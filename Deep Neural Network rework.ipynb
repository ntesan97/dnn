{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOXkxud4mqXq"
      },
      "source": [
        "# L layered Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CVATnCkmqXr"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dpVVXnCymqXs"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1RU_ceKmqXs"
      },
      "source": [
        "# Model\n",
        "\n",
        "![img](https://github.com/Coding-Lane/Deep-Neural-Network-from-scratch/blob/main/images/deep_network.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "segnae5nmqXt"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2LJ2ChdmqXt",
        "outputId": "50d52577-be2e-4a72-dc7d-f002b2c681f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12288, 209)\n",
            "(1, 209)\n",
            "(12288, 50)\n",
            "(1, 50)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import io\n",
        "\n",
        "def load_data_from_github(url):\n",
        "    \"\"\"Downloads a file from a given GitHub raw URL and loads it into a NumPy array.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    return np.loadtxt(io.StringIO(response.text), delimiter=',')\n",
        "\n",
        "# Base URL of the dataset directory in the GitHub repository\n",
        "base_url = 'https://raw.githubusercontent.com/ntesan97/dnn/main/dataset/'\n",
        "\n",
        "# Load training data\n",
        "X_train = load_data_from_github(base_url + 'cat_train_x.csv') / 255.0\n",
        "Y_train = load_data_from_github(base_url + 'cat_train_y.csv').reshape(1, X_train.shape[1])\n",
        "\n",
        "# Load testing data\n",
        "X_test = load_data_from_github(base_url + 'cat_test_x.csv') / 255.0\n",
        "Y_test = load_data_from_github(base_url + 'cat_test_y.csv').reshape(1, X_test.shape[1])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "9gI9cWBEmqXt",
        "outputId": "1fecc95e-6b29-4e1f-ff27-efcb1e49a8eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "index = random.randrange(0, X_train.shape[1])\n",
        "plt.imshow(X_train[:, index].reshape(64,64, 3))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dszTZThcmqXt"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YnvpDUMlmqXt"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "def softmax(z):\n",
        "    expZ = np.exp(z)\n",
        "    return expZ/(np.sum(expZ, 0))\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    return A\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def derivative_relu(Z):\n",
        "    return np.array(Z > 0, dtype = 'float')\n",
        "\n",
        "def derivative_tanh(x):\n",
        "    return (1 - np.power(x, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxqU9WMPmqXu"
      },
      "source": [
        "# Initialize Parameters\n",
        "\n",
        "- We need to initialize the **W** parameters randomly, and **B** with zeros\n",
        "- And as our Deep Neural network has **L layers**, we will repeat it for **L-1 times**, from $W_1 to W_L$\n",
        "\n",
        "<img src=\"https://github.com/Coding-Lane/Deep-Neural-Network-from-scratch/blob/main/images/params.png?raw=1\" width=\"350\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PrhYtk_8mqXu"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1rSXbcLmqXu",
        "outputId": "064c96ec-b77b-41eb-a625-7bf7f6192597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of W1: (100, 12288)\n",
            "Shape of B1: (100, 1) \n",
            "\n",
            "Shape of W2: (200, 100)\n",
            "Shape of B2: (200, 1) \n",
            "\n",
            "Shape of W3: (1, 200)\n",
            "Shape of B3: (1, 1) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "layer_dims = [X_train.shape[0], 100, 200, Y_train.shape[0]]\n",
        "params = initialize_parameters(layer_dims)\n",
        "\n",
        "for l in range(1, len(layer_dims)):\n",
        "    print(\"Shape of W\" + str(l) + \":\", params['W' + str(l)].shape)\n",
        "    print(\"Shape of B\" + str(l) + \":\", params['b' + str(l)].shape, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML_ZZrD8mqXu"
      },
      "source": [
        "#### Expected :\n",
        "\n",
        "Shape of W1: (100, 12288)\n",
        "\n",
        "Shape of B1: (100, 1)\n",
        "\n",
        "Shape of W2: (200, 100)\n",
        "\n",
        "Shape of B2: (200, 1)\n",
        "\n",
        "Shape of W3: (1, 200)\n",
        "\n",
        "Shape of B3: (1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74Nx1pQemqXu"
      },
      "source": [
        "# Forward Propagation\n",
        "\n",
        "- sigmoid activation function will be used only at the last layer (output layer), while we will use relu/tanh for other layers\n",
        "\n",
        "<img src=\"https://github.com/Coding-Lane/Deep-Neural-Network-from-scratch/blob/main/images/forward_prop.png?raw=1\" width=\"300\"/>\n",
        "\n",
        "\n",
        "For f(x), you can use either tanh or ReLU activation function. But also use the derivative of the same for Backpropagation as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UCXewfIPmqXu"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(X, parameters, activation):\n",
        "\n",
        "    forward_cache = {}\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    forward_cache['A0'] = X\n",
        "\n",
        "    for l in range(1, L):\n",
        "        forward_cache['Z' + str(l)] = parameters['W' + str(l)].dot(forward_cache['A' + str(l-1)]) + parameters['b' + str(l)]\n",
        "\n",
        "        if activation == 'tanh':\n",
        "            forward_cache['A' + str(l)] = tanh(forward_cache['Z' + str(l)])\n",
        "        else:\n",
        "            forward_cache['A' + str(l)] = relu(forward_cache['Z' + str(l)])\n",
        "\n",
        "\n",
        "    forward_cache['Z' + str(L)] = parameters['W' + str(L)].dot(forward_cache['A' + str(L-1)]) + parameters['b' + str(L)]\n",
        "\n",
        "    if forward_cache['Z' + str(L)].shape[0] == 1:\n",
        "        forward_cache['A' + str(L)] = sigmoid(forward_cache['Z' + str(L)])\n",
        "    else :\n",
        "        forward_cache['A' + str(L)] = softmax(forward_cache['Z' + str(L)])\n",
        "\n",
        "    return forward_cache['A' + str(L)], forward_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFnL70m-mqXu",
        "outputId": "fa3dc71a-4825-4172-b268-4a757c13a302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of A0 : (12288, 209)\n",
            "Shape of A1 : (100, 209)\n",
            "Shape of A2 : (200, 209)\n",
            "Shape of A3 : (1, 209)\n"
          ]
        }
      ],
      "source": [
        "aL, forw_cache = forward_propagation(X_train, params, 'relu')\n",
        "\n",
        "for l in range(len(params)//2 + 1):\n",
        "    print(\"Shape of A\" + str(l) + \" :\", forw_cache['A' + str(l)].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfcb3ep-mqXu"
      },
      "source": [
        "#### Expected :\n",
        "\n",
        "Shape of A0 : (12288, 209)\n",
        "\n",
        "Shape of A1 : (100, 209)\n",
        "\n",
        "Shape of A2 : (200, 209)\n",
        "\n",
        "Shape of A3 : (1, 209)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM8KSp5jmqXu"
      },
      "source": [
        "# Cost Function\n",
        "\n",
        "#### For binary classification:\n",
        "$ Cost = - \\frac{1}{m} \\sum_{i=1}^{m} [ y*log(a_L) + (1-y)*log(1 - a_L) ] $\n",
        "\n",
        "#### For multi-class classification:\n",
        "\n",
        "$ Cost = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{i=k}^{n}[ y_k*log(a_k) ]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rtO40nAdmqXv"
      },
      "outputs": [],
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    if Y.shape[0] == 1:\n",
        "        cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
        "    else:\n",
        "        cost = -(1./m) * np.sum(Y * np.log(AL))\n",
        "\n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMTSz5TemqXv"
      },
      "source": [
        "# Backward Propagation\n",
        "\n",
        "- For last layer, **$dZ_L$** will be $A_L - Y$\n",
        "- Except for last layer, we use a loop to implement backprop for other layers\n",
        "\n",
        "<img src=\"https://github.com/Coding-Lane/Deep-Neural-Network-from-scratch/blob/main/images/backward_prop.png?raw=1\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XrmUMP2gmqXv"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(AL, Y, parameters, forward_cache, activation):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(parameters)//2\n",
        "    m = AL.shape[1]\n",
        "\n",
        "    grads[\"dZ\" + str(L)] = AL - Y\n",
        "    grads[\"dW\" + str(L)] = 1./m * np.dot(grads[\"dZ\" + str(L)],forward_cache['A' + str(L-1)].T)\n",
        "    grads[\"db\" + str(L)] = 1./m * np.sum(grads[\"dZ\" + str(L)], axis = 1, keepdims = True)\n",
        "\n",
        "    for l in reversed(range(1, L)):\n",
        "        if activation == 'tanh':\n",
        "            grads[\"dZ\" + str(l)] = np.dot(parameters['W' + str(l+1)].T,grads[\"dZ\" + str(l+1)])*derivative_tanh(forward_cache['A' + str(l)])\n",
        "        else:\n",
        "            grads[\"dZ\" + str(l)] = np.dot(parameters['W' + str(l+1)].T,grads[\"dZ\" + str(l+1)])*derivative_relu(forward_cache['A' + str(l)])\n",
        "\n",
        "        grads[\"dW\" + str(l)] = 1./m * np.dot(grads[\"dZ\" + str(l)],forward_cache['A' + str(l-1)].T)\n",
        "        grads[\"db\" + str(l)] = 1./m * np.sum(grads[\"dZ\" + str(l)], axis = 1, keepdims = True)\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-kAW--amqXv",
        "outputId": "d53accc7-2ef6-49ab-ee7b-9984c90c7089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dZ3 : (1, 209)\n",
            "Shape of dW3 : (1, 200)\n",
            "Shape of dB3 : (1, 1) \n",
            "\n",
            "Shape of dZ2 : (200, 209)\n",
            "Shape of dW2 : (200, 100)\n",
            "Shape of dB2 : (200, 1) \n",
            "\n",
            "Shape of dZ1 : (100, 209)\n",
            "Shape of dW1 : (100, 12288)\n",
            "Shape of dB1 : (100, 1) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "grads = backward_propagation(forw_cache[\"A\" + str(3)], Y_train, params, forw_cache, 'relu')\n",
        "\n",
        "for l in reversed(range(1, len(grads)//3 + 1)):\n",
        "    print(\"Shape of dZ\" + str(l) + \" :\", grads['dZ' + str(l)].shape)\n",
        "    print(\"Shape of dW\" + str(l) + \" :\", grads['dW' + str(l)].shape)\n",
        "    print(\"Shape of dB\" + str(l) + \" :\", grads['db' + str(l)].shape, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk_Yo_9amqXv"
      },
      "source": [
        "#### Expected Output\n",
        "\n",
        "Shape of dZ3 : (1, 209)\n",
        "\n",
        "Shape of dW3 : (1, 200)\n",
        "\n",
        "Shape of dB3 : (1, 1)\n",
        "\n",
        "Shape of dZ2 : (200, 209)\n",
        "\n",
        "Shape of dW2 : (200, 100)\n",
        "\n",
        "Shape of dB2 : (200, 1)\n",
        "\n",
        "Shape of dZ1 : (100, 209)\n",
        "\n",
        "Shape of dW1 : (100, 12288)\n",
        "\n",
        "Shape of dB1 : (100, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Revxha0YmqXv"
      },
      "source": [
        "# Update Parameters\n",
        "<img src=\"https://github.com/Coding-Lane/Deep-Neural-Network-from-scratch/blob/main/images/update_params.png?raw=1\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wf9cDiarmqXv"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y67Edoq1mqXv"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "am7-7K0jmqXv"
      },
      "outputs": [],
      "source": [
        "def predict(X, y, parameters, activation):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    y_pred, caches = forward_propagation(X, parameters, activation)\n",
        "\n",
        "    if y.shape[0] == 1:\n",
        "        y_pred = np.array(y_pred > 0.5, dtype = 'float')\n",
        "    else:\n",
        "        y = np.argmax(y, 0)\n",
        "        y_pred = np.argmax(y_pred, 0)\n",
        "\n",
        "    return np.round(np.sum((y_pred == y)/m), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDU2lpjgmqXv"
      },
      "source": [
        "# Complete Model\n",
        "\n",
        "Implement the entire Deep Neural Network here\n",
        "\n",
        "### Instructions :\n",
        "\n",
        "We need to initialize parameters once, and after that, we will run the following in a loop:\n",
        "- forward_prop(x, parameters)\n",
        "- cost_function(aL, y)\n",
        "- backward_prop(x, y, parameters, forward_cache)\n",
        "- parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "### Return :\n",
        "- parameters, which will be our trained parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "q_mflWBSmqXv"
      },
      "outputs": [],
      "source": [
        "def model(X, Y, layers_dims, learning_rate = 0.03, activation = 'relu', num_iterations = 3000):#lr was 0.009\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []\n",
        "\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        AL, forward_cache = forward_propagation(X, parameters, activation)\n",
        "\n",
        "        cost = compute_cost(AL, Y)\n",
        "\n",
        "        grads = backward_propagation(AL, Y, parameters, forward_cache, activation)\n",
        "\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        if i % (num_iterations/10) == 0:\n",
        "            print(\"\\niter:{} \\t cost: {} \\t train_acc:{} \\t test_acc:{}\".format(i, np.round(cost, 2), predict(X_train, Y_train, parameters, activation), predict(X_test, Y_test, parameters, activation)))\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(\"==\", end = '')\n",
        "\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q46I3SgNmqXv",
        "outputId": "13e6221a-0a44-4b55-8464-a9e7e198eed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "iter:0 \t cost: 0.77 \t train_acc:0.51 \t test_acc:0.42\n",
            "==================================================\n",
            "iter:250 \t cost: 0.63 \t train_acc:0.74 \t test_acc:0.64\n",
            "==================================================\n",
            "iter:500 \t cost: 0.54 \t train_acc:0.78 \t test_acc:0.7\n",
            "==================================================\n",
            "iter:750 \t cost: 0.44 \t train_acc:0.92 \t test_acc:0.78\n",
            "==================================================\n",
            "iter:1000 \t cost: 0.32 \t train_acc:0.96 \t test_acc:0.8\n",
            "==================================================\n",
            "iter:1250 \t cost: 0.23 \t train_acc:0.98 \t test_acc:0.76\n",
            "==================================================\n",
            "iter:1500 \t cost: 0.16 \t train_acc:0.98 \t test_acc:0.82\n",
            "==================================================\n",
            "iter:1750 \t cost: 0.13 \t train_acc:0.98 \t test_acc:0.8\n",
            "==================================================\n",
            "iter:2000 \t cost: 0.11 \t train_acc:0.98 \t test_acc:0.8\n",
            "==================================================\n",
            "iter:2250 \t cost: 0.1 \t train_acc:0.98 \t test_acc:0.8\n",
            "=================================================="
          ]
        }
      ],
      "source": [
        "layers_dims = [X_train.shape[0], 20, 7, 5, Y_train.shape[0]] #  4-layer model\n",
        "lr = 0.0075\n",
        "iters = 2500\n",
        "\n",
        "parameters = model(X_train, Y_train, layers_dims, learning_rate = lr, activation = 'relu', num_iterations = iters)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1AaJjFr8p9bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61bf1826"
      },
      "source": [
        "# Task\n",
        "Explain what the provided Jupyter notebook does, add educational comments to all code cells, and summarize the key components of the implemented neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c61a9be"
      },
      "source": [
        "## Provide a general overview of the notebook's purpose.\n",
        "\n",
        "### Subtask:\n",
        "Provide a general overview of the notebook's purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e155fa24"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the markdown and code cells, the notebook implements a deep neural network for image classification. I will now provide a concise description of the notebook's purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af273cc1"
      },
      "source": [
        "# The notebook implements a layered deep neural network from scratch using NumPy.\n",
        "# It covers the fundamental building blocks of a neural network, including:\n",
        "# - Data loading and preprocessing (specifically for a cat vs non-cat image dataset).\n",
        "# - Initialization of parameters (weights and biases).\n",
        "# - Implementation of activation functions (sigmoid, softmax, ReLU, tanh) and their derivatives.\n",
        "# - Forward propagation to compute the output of the network.\n",
        "# - Computation of the cost function (binary and multi-class cross-entropy).\n",
        "# - Backward propagation to calculate gradients.\n",
        "# - Updating parameters using gradient descent.\n",
        "# - Making predictions and evaluating accuracy.\n",
        "# The notebook then demonstrates training the implemented model on the loaded dataset and shows the progress of the training process by printing the cost and accuracy at intervals."
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f9260e7"
      },
      "source": [
        "## Add educational comments to each code cell explaining its functionality.\n",
        "\n",
        "### Subtask:\n",
        "Add educational comments to each code cell explaining its functionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbe1e817"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell that imports necessary libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0960aa05"
      },
      "source": [
        "# Import necessary libraries for the deep learning model.\n",
        "import time # Used for timing operations, though not explicitly used in the provided code.\n",
        "import random # Used for generating random numbers, specifically for selecting a random image to display.\n",
        "import numpy as np # Fundamental library for numerical operations, essential for matrix manipulations in neural networks.\n",
        "import matplotlib.pyplot as plt # Library for plotting graphs, used here to display an image.\n",
        "# from tensorflow.keras.datasets import mnist # This import is commented out and not used in the current notebook.\n",
        "# from keras.utils import to_categorical # This import is commented out and not used in the current notebook."
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ded9064a"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell that loads the dataset from GitHub.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abe8e2a7",
        "outputId": "f585bef1-089f-4937-98fb-4c942e472a18"
      },
      "source": [
        "# Import requests for downloading data from a URL and io for working with in-memory text files.\n",
        "import requests\n",
        "import io\n",
        "\n",
        "def load_data_from_github(url):\n",
        "    \"\"\"Downloads a file from a given GitHub raw URL and loads it into a NumPy array.\"\"\"\n",
        "    # Send a GET request to the specified URL to download the data.\n",
        "    response = requests.get(url)\n",
        "    # Raise an HTTPError for bad responses (4xx or 5xx).\n",
        "    response.raise_for_status()\n",
        "    # Read the content of the response into a string and then load it into a NumPy array,\n",
        "    # assuming the data is comma-separated values.\n",
        "    return np.loadtxt(io.StringIO(response.text), delimiter=',')\n",
        "\n",
        "# Base URL of the dataset directory in the GitHub repository where the data files are stored.\n",
        "base_url = 'https://raw.githubusercontent.com/ntesan97/dnn/main/dataset/'\n",
        "\n",
        "# Load training data for features (X_train) and labels (Y_train).\n",
        "# X_train is normalized by dividing by 255.0 to scale pixel values between 0 and 1.\n",
        "X_train = load_data_from_github(base_url + 'cat_train_x.csv') / 255.0\n",
        "# Y_train is loaded and reshaped to have a shape of (1, number of samples) to match the expected label format.\n",
        "Y_train = load_data_from_github(base_url + 'cat_train_y.csv').reshape(1, X_train.shape[1])\n",
        "\n",
        "# Load testing data for features (X_test) and labels (Y_test) following the same process as training data.\n",
        "X_test = load_data_from_github(base_url + 'cat_test_x.csv') / 255.0\n",
        "Y_test = load_data_from_github(base_url + 'cat_test_y.csv').reshape(1, X_test.shape[1])\n",
        "\n",
        "# Print the shapes of the loaded datasets to verify they are loaded correctly.\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12288, 209)\n",
            "(1, 209)\n",
            "(12288, 50)\n",
            "(1, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9662f5be"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell that displays a random training image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "d952c196",
        "outputId": "9aa1c609-bf39-4d73-9cef-743218cf4949"
      },
      "source": [
        "# Select a random index from the range of the number of training examples.\n",
        "index = random.randrange(0, X_train.shape[1])\n",
        "# Reshape the selected image data from a flattened vector into a 64x64 pixel, 3-channel (RGB) image\n",
        "# and display it using matplotlib's imshow function.\n",
        "plt.imshow(X_train[:, index].reshape(64,64, 3))\n",
        "# Show the plotted image.\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "767fe61c"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell defining activation functions and their derivatives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e147999"
      },
      "source": [
        "# Define the sigmoid activation function, which squashes values between 0 and 1.\n",
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "# Define the softmax activation function, used for multi-class classification\n",
        "# to convert raw scores into probabilities that sum to 1.\n",
        "def softmax(z):\n",
        "    expZ = np.exp(z)\n",
        "    return expZ/(np.sum(expZ, 0))\n",
        "\n",
        "# Define the ReLU (Rectified Linear Unit) activation function, which returns the input\n",
        "# if it's positive, and 0 otherwise.\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    return A\n",
        "\n",
        "# Define the tanh (Hyperbolic Tangent) activation function, which squashes values\n",
        "# between -1 and 1.\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Define the derivative of the ReLU function, used in backpropagation.\n",
        "# The derivative is 1 for positive inputs and 0 for non-positive inputs.\n",
        "def derivative_relu(Z):\n",
        "    return np.array(Z > 0, dtype = 'float')\n",
        "\n",
        "# Define the derivative of the tanh function, used in backpropagation.\n",
        "# The derivative is 1 - tanh(x)^2.\n",
        "def derivative_tanh(x):\n",
        "    return (1 - np.power(x, 2))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3acc739b"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell that initializes the parameters of the neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ee24bd"
      },
      "source": [
        "# Define a function to initialize the weights (W) and biases (b) for each layer of the neural network.\n",
        "def initialize_parameters(layer_dims):\n",
        "\n",
        "    # Initialize an empty dictionary to store the parameters.\n",
        "    parameters = {}\n",
        "    # Get the number of layers in the network (L is the number of weight/bias pairs).\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    # Loop through each layer starting from the second layer (l=1) up to the last layer (L-1).\n",
        "    for l in range(1, L):\n",
        "        # Initialize weights for the current layer 'l'.\n",
        "        # Weights are initialized randomly from a standard normal distribution.\n",
        "        # The scale is divided by the square root of the number of neurons in the previous layer (layer_dims[l-1]).\n",
        "        # This is a common technique (He initialization for ReLU) to help prevent vanishing/exploding gradients.\n",
        "        # The shape of W[l] is (number of neurons in current layer, number of neurons in previous layer).\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
        "        # Initialize biases for the current layer 'l' with zeros.\n",
        "        # The shape of b[l] is (number of neurons in current layer, 1).\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    # Return the dictionary containing the initialized parameters.\n",
        "    return parameters"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5916a957"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell that tests the parameter initialization function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1e77ff",
        "outputId": "636f973f-b889-499b-d614-c167e8b6536b"
      },
      "source": [
        "# Define the dimensions of each layer in the neural network.\n",
        "# The first element is the input layer size (number of features),\n",
        "# the intermediate elements are the number of neurons in the hidden layers,\n",
        "# and the last element is the output layer size.\n",
        "layer_dims = [X_train.shape[0], 100, 200, Y_train.shape[0]]\n",
        "# Initialize the parameters of the neural network using the defined layer dimensions.\n",
        "params = initialize_parameters(layer_dims)\n",
        "\n",
        "# Loop through each layer (starting from the second layer, index 1) to print the shapes\n",
        "# of the initialized weight and bias matrices to verify they are correct.\n",
        "for l in range(1, len(layer_dims)):\n",
        "    print(\"Shape of W\" + str(l) + \":\", params['W' + str(l)].shape)\n",
        "    print(\"Shape of B\" + str(l) + \":\", params['b' + str(l)].shape, \"\\n\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of W1: (100, 12288)\n",
            "Shape of B1: (100, 1) \n",
            "\n",
            "Shape of W2: (200, 100)\n",
            "Shape of B2: (200, 1) \n",
            "\n",
            "Shape of W3: (1, 200)\n",
            "Shape of B3: (1, 1) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "756f8d2d"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell implementing the forward propagation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "813eeb5d"
      },
      "source": [
        "# Define the forward propagation function to compute the output of the neural network.\n",
        "# X: Input data.\n",
        "# parameters: Dictionary containing the weights and biases for each layer.\n",
        "# activation: The activation function to use for the hidden layers ('relu' or 'tanh').\n",
        "def forward_propagation(X, parameters, activation):\n",
        "\n",
        "    # Initialize a dictionary to store the activation values (A) and pre-activation values (Z)\n",
        "    # for each layer during the forward pass. These are needed for backpropagation.\n",
        "    forward_cache = {}\n",
        "    # Calculate the number of layers with learnable parameters (L).\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    # Store the input data X as the activation of the input layer (A0).\n",
        "    forward_cache['A0'] = X\n",
        "\n",
        "    # Loop through the hidden layers (from layer 1 to L-1).\n",
        "    for l in range(1, L):\n",
        "        # Retrieve the weights (W) and biases (b) for the current layer.\n",
        "        W = parameters['W' + str(l)]\n",
        "        b = parameters['b' + str(l)]\n",
        "        # Calculate the pre-activation value (Z) for the current layer: Z[l] = W[l] * A[l-1] + b[l].\n",
        "        forward_cache['Z' + str(l)] = np.dot(W, forward_cache['A' + str(l-1)]) + b\n",
        "\n",
        "        # Apply the specified activation function ('relu' or 'tanh') to the pre-activation value\n",
        "        # to get the activation value (A) for the current layer.\n",
        "        if activation == 'tanh':\n",
        "            forward_cache['A' + str(l)] = tanh(forward_cache['Z' + str(l)])\n",
        "        else:\n",
        "            forward_cache['A' + str(l)] = relu(forward_cache['Z' + str(l)])\n",
        "\n",
        "    # Handle the output layer (layer L).\n",
        "    # Calculate the pre-activation value (Z) for the output layer.\n",
        "    forward_cache['Z' + str(L)] = np.dot(parameters['W' + str(L)], forward_cache['A' + str(L-1)]) + parameters['b' + str(L)]\n",
        "\n",
        "    # Apply the appropriate activation function for the output layer.\n",
        "    # If the output layer has only one neuron (binary classification), use sigmoid.\n",
        "    # If the output layer has more than one neuron (multi-class classification), use softmax.\n",
        "    if forward_cache['Z' + str(L)].shape[0] == 1:\n",
        "        forward_cache['A' + str(L)] = sigmoid(forward_cache['Z' + str(L)])\n",
        "    else :\n",
        "        forward_cache['A' + str(L)] = softmax(forward_cache['Z' + str(L)])\n",
        "\n",
        "    # Return the final output activation (AL) and the cache containing all intermediate values.\n",
        "    return forward_cache['A' + str(L)], forward_cache"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af2fc669"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell that tests the forward propagation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c9183f3",
        "outputId": "09794726-6e5a-4576-d155-4269cda96d4d"
      },
      "source": [
        "# Perform forward propagation on the training data using the initialized parameters and ReLU activation for hidden layers.\n",
        "aL, forw_cache = forward_propagation(X_train, params, 'relu')\n",
        "\n",
        "# Loop through the layers (including the input layer A0) and print the shapes\n",
        "# of the activation values (A) stored in the forward_cache to verify the dimensions\n",
        "# after each layer's computation.\n",
        "for l in range(len(params)//2 + 1):\n",
        "    print(\"Shape of A\" + str(l) + \" :\", forw_cache['A' + str(l)].shape)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of A0 : (12288, 209)\n",
            "Shape of A1 : (100, 209)\n",
            "Shape of A2 : (200, 209)\n",
            "Shape of A3 : (1, 209)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07051ec0"
      },
      "source": [
        "## Add educational comments to each code cell explaining its functionality.\n",
        "\n",
        "### Subtask:\n",
        "Add educational comments to each code cell explaining its functionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d33a960"
      },
      "source": [
        "**Reasoning**:\n",
        "Add detailed educational comments to the code cell that computes the cost function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b33e8ce"
      },
      "source": [
        "# Define the cost function to evaluate the performance of the neural network.\n",
        "# AL: The predicted output of the neural network (activation of the last layer).\n",
        "# Y: The true labels.\n",
        "def compute_cost(AL, Y):\n",
        "    # Get the number of training examples (m).\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute the cost based on the type of classification task.\n",
        "    # If the output layer has only one neuron (Y.shape[0] == 1), it's binary classification.\n",
        "    # The binary cross-entropy loss is calculated.\n",
        "    if Y.shape[0] == 1:\n",
        "        # Formula: -(1/m) * sum(y*log(AL) + (1-y)*log(1-AL))\n",
        "        cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
        "    # If the output layer has more than one neuron, it's multi-class classification.\n",
        "    # The categorical cross-entropy loss is calculated.\n",
        "    else:\n",
        "        # Formula: -(1/m) * sum(Y * log(AL))\n",
        "        # Note: Y is expected to be one-hot encoded for this formula.\n",
        "        cost = -(1./m) * np.sum(Y * np.log(AL))\n",
        "\n",
        "    # Ensure the cost is a scalar value (e.g., convert [[17]] to 17).\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    # Return the computed cost.\n",
        "    return cost"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc8e67ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Add detailed educational comments to the code cell that implements backward propagation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73f0c74c"
      },
      "source": [
        "# Define the backward propagation function to compute the gradients of the cost\n",
        "# with respect to the weights and biases.\n",
        "# AL: The predicted output (activation of the last layer).\n",
        "# Y: The true labels.\n",
        "# parameters: Dictionary containing the weights and biases.\n",
        "# forward_cache: Dictionary containing intermediate values from the forward pass (A and Z values).\n",
        "# activation: The activation function used in hidden layers ('relu' or 'tanh').\n",
        "def backward_propagation(AL, Y, parameters, forward_cache, activation):\n",
        "\n",
        "    # Initialize a dictionary to store the gradients.\n",
        "    grads = {}\n",
        "    # Get the number of layers with learnable parameters (L).\n",
        "    L = len(parameters)//2\n",
        "    # Get the number of training examples (m).\n",
        "    m = AL.shape[1]\n",
        "\n",
        "    # --- Backward pass for the output layer (Layer L) ---\n",
        "    # Calculate the gradient of the cost with respect to the pre-activation value of the output layer (dZ[L]).\n",
        "    # For binary cross-entropy or categorical cross-entropy with sigmoid/softmax output, dZ[L] = AL - Y.\n",
        "    grads[\"dZ\" + str(L)] = AL - Y\n",
        "    # Calculate the gradient of the cost with respect to the weights of the output layer (dW[L]).\n",
        "    # dW[L] = (1/m) * dZ[L] * A[L-1].T\n",
        "    grads[\"dW\" + str(L)] = 1./m * np.dot(grads[\"dZ\" + str(L)],forward_cache['A' + str(L-1)].T)\n",
        "    # Calculate the gradient of the cost with respect to the biases of the output layer (db[L]).\n",
        "    # db[L] = (1/m) * sum(dZ[L]) over all examples.\n",
        "    grads[\"db\" + str(L)] = 1./m * np.sum(grads[\"dZ\" + str(L)], axis = 1, keepdims = True)\n",
        "\n",
        "    # --- Backward pass for the hidden layers (from L-1 down to 1) ---\n",
        "    # Loop through the hidden layers in reverse order.\n",
        "    for l in reversed(range(1, L)):\n",
        "        # Calculate the gradient of the cost with respect to the pre-activation value of the current layer (dZ[l]).\n",
        "        # This involves backpropagating the gradient from the next layer (dZ[l+1])\n",
        "        # and multiplying by the derivative of the activation function of the current layer.\n",
        "        # dZ[l] = (W[l+1].T * dZ[l+1]) * derivative(A[l]).\n",
        "        if activation == 'tanh':\n",
        "            grads[\"dZ\" + str(l)] = np.dot(parameters['W' + str(l+1)].T,grads[\"dZ\" + str(l+1)])*derivative_tanh(forward_cache['A' + str(l)])\n",
        "        else: # activation == 'relu'\n",
        "            grads[\"dZ\" + str(l)] = np.dot(parameters['W' + str(l+1)].T,grads[\"dZ\" + str(l+1)])*derivative_relu(forward_cache['A' + str(l)])\n",
        "\n",
        "        # Calculate the gradient of the cost with respect to the weights of the current layer (dW[l]).\n",
        "        # dW[l] = (1/m) * dZ[l] * A[l-1].T\n",
        "        grads[\"dW\" + str(l)] = 1./m * np.dot(grads[\"dZ\" + str(l)],forward_cache['A' + str(l-1)].T)\n",
        "        # Calculate the gradient of the cost with respect to the biases of the current layer (db[l]).\n",
        "        # db[l] = (1/m) * sum(dZ[l]) over all examples.\n",
        "        grads[\"db\" + str(l)] = 1./m * np.sum(grads[\"dZ\" + str(l)], axis = 1, keepdims = True)\n",
        "\n",
        "    # Return the dictionary containing the computed gradients.\n",
        "    return grads"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6af8cadc"
      },
      "source": [
        "**Reasoning**:\n",
        "Add detailed educational comments to the code cell that tests the backward propagation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d397501",
        "outputId": "138ad83d-d3f9-4416-b239-ce4b2ff03254"
      },
      "source": [
        "# Test the backward propagation function.\n",
        "# Compute the gradients using the output activation (A3), true labels (Y_train),\n",
        "# initialized parameters (params), forward cache (forw_cache), and 'relu' activation.\n",
        "grads = backward_propagation(forw_cache[\"A\" + str(3)], Y_train, params, forw_cache, 'relu')\n",
        "\n",
        "# Loop through the layers in reverse order (from the last layer down to 1)\n",
        "# and print the shapes of the computed gradients (dZ, dW, and db) to verify their dimensions.\n",
        "for l in reversed(range(1, len(grads)//3 + 1)):\n",
        "    print(\"Shape of dZ\" + str(l) + \" :\", grads['dZ' + str(l)].shape)\n",
        "    print(\"Shape of dW\" + str(l) + \" :\", grads['dW' + str(l)].shape)\n",
        "    print(\"Shape of dB\" + str(l) + \" :\", grads['db' + str(l)].shape, \"\\n\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dZ3 : (1, 209)\n",
            "Shape of dW3 : (1, 200)\n",
            "Shape of dB3 : (1, 1) \n",
            "\n",
            "Shape of dZ2 : (200, 209)\n",
            "Shape of dW2 : (200, 100)\n",
            "Shape of dB2 : (200, 1) \n",
            "\n",
            "Shape of dZ1 : (100, 209)\n",
            "Shape of dW1 : (100, 12288)\n",
            "Shape of dB1 : (100, 1) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d319c4"
      },
      "source": [
        "**Reasoning**:\n",
        "Add detailed educational comments to the code cell that updates the parameters using gradient descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abd4dd70"
      },
      "source": [
        "# Define the function to update the neural network's parameters (weights and biases)\n",
        "# using the gradients computed during backward propagation and a specified learning rate.\n",
        "# parameters: Dictionary containing the current weights and biases.\n",
        "# grads: Dictionary containing the computed gradients (dW and db) for each layer.\n",
        "# learning_rate: The learning rate, a hyperparameter that controls the step size\n",
        "#                during parameter updates.\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    # Get the number of layers with learnable parameters (L).\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    # Loop through each layer (from 0 to L-1).\n",
        "    for l in range(L):\n",
        "        # Update the weights for the current layer 'l+1'.\n",
        "        # New W = Old W - learning_rate * dW.\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        # Update the biases for the current layer 'l+1'.\n",
        "        # New b = Old b - learning_rate * db.\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "\n",
        "    # Return the dictionary containing the updated parameters.\n",
        "    return parameters"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57bef894"
      },
      "source": [
        "**Reasoning**:\n",
        "Add detailed educational comments to the code cell that makes predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6482aaaa"
      },
      "source": [
        "# Define the function to make predictions using the trained neural network.\n",
        "# X: Input data for making predictions.\n",
        "# y: True labels (used for calculating accuracy).\n",
        "# parameters: Dictionary containing the trained weights and biases.\n",
        "# activation: The activation function used in hidden layers ('relu' or 'tanh').\n",
        "def predict(X, y, parameters, activation):\n",
        "\n",
        "    # Get the number of examples in the input data (m).\n",
        "    m = X.shape[1]\n",
        "    # Perform forward propagation on the input data to get the predicted output (y_pred)\n",
        "    # and the forward cache (though the cache is not used in this function).\n",
        "    y_pred, caches = forward_propagation(X, parameters, activation)\n",
        "\n",
        "    # Convert the predicted probabilities into binary predictions (0 or 1) for binary classification.\n",
        "    # If the true labels y have only one row, it's binary classification.\n",
        "    # Predictions are rounded to 0 or 1 based on a threshold of 0.5.\n",
        "    if y.shape[0] == 1:\n",
        "        y_pred = np.array(y_pred > 0.5, dtype = 'float')\n",
        "    # For multi-class classification, convert predicted probabilities (output of softmax)\n",
        "    # and true labels (if not already) into class indices by finding the index of the maximum value.\n",
        "    else:\n",
        "        # Assuming y is not one-hot encoded and contains class indices.\n",
        "        # If y is one-hot encoded, this line might need adjustment.\n",
        "        y = np.argmax(y, 0)\n",
        "        # Get the predicted class index for each example.\n",
        "        y_pred = np.argmax(y_pred, 0)\n",
        "\n",
        "    # Calculate the accuracy by comparing predicted labels (y_pred) with true labels (y).\n",
        "    # Sum the number of correct predictions and divide by the total number of examples (m).\n",
        "    # Round the accuracy to 2 decimal places.\n",
        "    return np.round(np.sum((y_pred == y)/m), 2)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60c353c9"
      },
      "source": [
        "**Reasoning**:\n",
        "Add detailed educational comments to the code cell that implements the complete deep neural network model training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9391b414"
      },
      "source": [
        "# Define the main model function that orchestrates the entire deep neural network training process.\n",
        "# X: Training data features.\n",
        "# Y: Training data labels.\n",
        "# layers_dims: A list specifying the number of neurons in each layer (input, hidden, output).\n",
        "# learning_rate: The learning rate for parameter updates (default is 0.03).\n",
        "# activation: The activation function to use for hidden layers ('relu' or 'tanh', default is 'relu').\n",
        "# num_iterations: The number of training iterations (epochs) to run (default is 3000).\n",
        "def model(X, Y, layers_dims, learning_rate = 0.03, activation = 'relu', num_iterations = 3000):#lr was 0.009\n",
        "\n",
        "    # Set a random seed for reproducibility of results.\n",
        "    np.random.seed(1)\n",
        "    # Initialize an empty list to store the cost at different iterations (for plotting later, though not plotted here).\n",
        "    costs = []\n",
        "\n",
        "    # Initialize the parameters (weights and biases) of the neural network based on the layer dimensions.\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    # Start the training loop for the specified number of iterations.\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # --- Forward Propagation ---\n",
        "        # Perform forward propagation to compute the output of the network (AL)\n",
        "        # and store intermediate values in the forward_cache.\n",
        "        AL, forward_cache = forward_propagation(X, parameters, activation)\n",
        "\n",
        "        # --- Compute Cost ---\n",
        "        # Calculate the cost (loss) based on the predicted output (AL) and true labels (Y).\n",
        "        cost = compute_cost(AL, Y)\n",
        "\n",
        "        # --- Backward Propagation ---\n",
        "        # Perform backward propagation to compute the gradients of the cost\n",
        "        # with respect to the parameters.\n",
        "        grads = backward_propagation(AL, Y, parameters, forward_cache, activation)\n",
        "\n",
        "        # --- Update Parameters ---\n",
        "        # Update the parameters (weights and biases) using the computed gradients\n",
        "        # and the specified learning rate.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # --- Print Progress ---\n",
        "        # Print the current iteration number, cost, and training/testing accuracy at regular intervals.\n",
        "        # This helps monitor the training progress. The interval is set to 1/10th of the total iterations.\n",
        "        if i % (num_iterations/10) == 0:\n",
        "            print(\"\\niter:{} \\t cost: {} \\t train_acc:{} \\t test_acc:{}\".format(i, np.round(cost, 2), predict(X_train, Y_train, parameters, activation), predict(X_test, Y_test, parameters, activation)))\n",
        "\n",
        "        # Print \"==\" to show progress visually every 10 iterations.\n",
        "        if i % 10 == 0:\n",
        "            print(\"==\", end = '')\n",
        "\n",
        "\n",
        "    # Return the trained parameters after completing all iterations.\n",
        "    return parameters"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fa02a42"
      },
      "source": [
        "**Reasoning**:\n",
        "Add detailed educational comments to the code cell that trains the model and prints the final parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fa721fa",
        "outputId": "391e4180-df50-4e84-c005-ca39080c89c3"
      },
      "source": [
        "# Define the dimensions for a 4-layer neural network (input -> hidden1 -> hidden2 -> hidden3 -> output).\n",
        "# X_train.shape[0]: Input layer size (number of features).\n",
        "# 20, 7, 5: Number of neurons in the three hidden layers.\n",
        "# Y_train.shape[0]: Output layer size (number of output neurons).\n",
        "layers_dims = [X_train.shape[0], 20, 7, 5, Y_train.shape[0]] #  4-layer model\n",
        "# Set the learning rate for the gradient descent algorithm.\n",
        "lr = 0.0075\n",
        "# Set the total number of training iterations (epochs).\n",
        "iters = 2500\n",
        "\n",
        "# Train the model using the defined layers_dims, learning rate, and number of iterations.\n",
        "# The model function executes the forward propagation, cost computation, backward propagation,\n",
        "# and parameter update steps for the specified number of iterations.\n",
        "# The trained parameters are returned and stored in the 'parameters' variable.\n",
        "parameters = model(X_train, Y_train, layers_dims, learning_rate = lr, activation = 'relu', num_iterations = iters)\n",
        "\n",
        "# After training, the 'parameters' dictionary contains the optimized weights and biases\n",
        "# that the model learned from the training data. These parameters can then be used\n",
        "# to make predictions on new, unseen data.\n",
        "# Although not explicitly printed here, the 'parameters' variable holds the final trained model."
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "iter:0 \t cost: 0.77 \t train_acc:0.51 \t test_acc:0.42\n",
            "==================================================\n",
            "iter:250 \t cost: 0.63 \t train_acc:0.74 \t test_acc:0.64\n",
            "==================================================\n",
            "iter:500 \t cost: 0.54 \t train_acc:0.78 \t test_acc:0.7\n",
            "==================================================\n",
            "iter:750 \t cost: 0.44 \t train_acc:0.92 \t test_acc:0.78\n",
            "==================================================\n",
            "iter:1000 \t cost: 0.32 \t train_acc:0.96 \t test_acc:0.8\n",
            "==================================================\n",
            "iter:1250 \t cost: 0.23 \t train_acc:0.98 \t test_acc:0.76\n",
            "==================================================\n",
            "iter:1500 \t cost: 0.16 \t train_acc:0.98 \t test_acc:0.82\n",
            "==================================================\n",
            "iter:1750 \t cost: 0.13 \t train_acc:0.98 \t test_acc:0.8\n",
            "==================================================\n",
            "iter:2000 \t cost: 0.11 \t train_acc:0.98 \t test_acc:0.8\n",
            "==================================================\n",
            "iter:2250 \t cost: 0.1 \t train_acc:0.98 \t test_acc:0.8\n",
            "=================================================="
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8998631d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The notebook implements a deep neural network from scratch using NumPy, covering essential components like data loading, parameter initialization, activation functions (sigmoid, softmax, ReLU, tanh), forward and backward propagation, cost functions (binary and multi-class cross-entropy), and gradient descent for parameter updates.\n",
        "*   The training process, as demonstrated by the output, shows a decrease in cost and an increase in both training and testing accuracy over iterations, indicating that the model is learning.\n",
        "*   The shapes of the initialized parameters (weights and biases) and computed gradients (dZ, dW, and db) align with the defined layer dimensions of the neural network.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Further analysis could involve visualizing the cost function's decrease over iterations to assess the training convergence rate.\n",
        "*   Experimenting with different hyperparameters, such as the learning rate, number of iterations, and hidden layer activation functions (ReLU vs. Tanh), could potentially improve model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bf19653"
      },
      "source": [
        "# Task\n",
        "Modify the provided Python notebook code to create a deep learning model for a Bible chatbot project. The project aims to use a sample of Bible text. The modifications should include adapting the data loading and preprocessing for text data, adjusting the neural network architecture to handle sequential text, and outlining the training and text generation/retrieval process. The final output should explain how the notebook could be used for this project, including necessary code changes and educational comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e033c7"
      },
      "source": [
        "## Understand the task\n",
        "\n",
        "### Subtask:\n",
        "Clarify the specific type of chatbot (e.g., generative, retrieval-based) and the nature of the Bible text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db541d3b"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the task description which mentions \"text generation/retrieval process\" and the initial structure of the notebook which implements a basic feedforward neural network, it's not immediately clear if the goal is a generative or retrieval-based chatbot. However, the current model architecture is not directly suitable for sequence generation. A retrieval-based approach, where the model learns to identify relevant passages given a query, could potentially be adapted from a classification-like task if the output layer is modified. A generative model would typically require a different architecture (like RNNs or Transformers). Given the simplicity of the current model, a retrieval-based approach seems more feasible with modifications. The Bible text data consists of structured text (chapters, verses) with a specific narrative and linguistic style. This structure could be used for indexing and retrieval, while the content itself would be the input for the model to understand context or similarity for retrieval. I will clarify these points in a markdown cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "6uM0q8k5soIE",
        "outputId": "3fb2347a-8263-4065-a1d4-a79539ebc8d8"
      },
      "source": [
        "%%markdown\n",
        "## Clarify the specific type of chatbot (e.g., generative, retrieval-based) and the nature of the Bible text data.\n",
        "\n",
        "### Subtask:\n",
        "Clarify the specific type of chatbot (e.g., generative, retrieval-based) and the nature of the Bible text data.\n",
        "\n",
        "**Clarification:**\n",
        "\n",
        "Given the existing neural network architecture in the notebook, which is a feedforward deep neural network, it is more suitable for classification or regression tasks rather than sequence generation. While it could potentially be adapted for a simplified retrieval-based chatbot by formulating the problem as classifying the relevance of a passage to a query, a truly generative chatbot would typically require recurrent neural networks (RNNs), LSTMs, or Transformer-based architectures.\n",
        "\n",
        "For the purpose of adapting this notebook, we will assume the goal is a **retrieval-based chatbot**. This type of chatbot would likely work by:\n",
        "1. **Indexing Bible text:** Breaking down the Bible into smaller units (e.g., verses, chapters, or paragraphs) and creating a searchable index.\n",
        "2. **Processing user queries and Bible text:** Converting user questions and Bible passages into numerical representations (embeddings) using a neural network.\n",
        "3. **Retrieving relevant passages:** Comparing the query embedding to the embeddings of Bible passages to find the most similar or relevant ones.\n",
        "\n",
        "The **nature of the Bible text data** is sequential and structured, organized into books, chapters, and verses. It contains a rich vocabulary and a distinct linguistic style. For a retrieval-based chatbot, relevant aspects of this data include:\n",
        "- **Text content:** The actual words and sentences that will be encoded into numerical representations.\n",
        "- **Structural information:** Book, chapter, and verse numbers, which can be used to identify and present the retrieved passages.\n",
        "- **Context:** The surrounding verses or chapters that provide context for a particular passage.\n",
        "\n",
        "Adapting this notebook will involve modifying the data loading to handle text, preprocessing the text into a format suitable for the neural network (e.g., using embeddings), and adjusting the network's input and output layers to handle these text representations for the retrieval task."
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Clarify the specific type of chatbot (e.g., generative, retrieval-based) and the nature of the Bible text data.\n\n### Subtask:\nClarify the specific type of chatbot (e.g., generative, retrieval-based) and the nature of the Bible text data.\n\n**Clarification:**\n\nGiven the existing neural network architecture in the notebook, which is a feedforward deep neural network, it is more suitable for classification or regression tasks rather than sequence generation. While it could potentially be adapted for a simplified retrieval-based chatbot by formulating the problem as classifying the relevance of a passage to a query, a truly generative chatbot would typically require recurrent neural networks (RNNs), LSTMs, or Transformer-based architectures.\n\nFor the purpose of adapting this notebook, we will assume the goal is a **retrieval-based chatbot**. This type of chatbot would likely work by:\n1. **Indexing Bible text:** Breaking down the Bible into smaller units (e.g., verses, chapters, or paragraphs) and creating a searchable index.\n2. **Processing user queries and Bible text:** Converting user questions and Bible passages into numerical representations (embeddings) using a neural network.\n3. **Retrieving relevant passages:** Comparing the query embedding to the embeddings of Bible passages to find the most similar or relevant ones.\n\nThe **nature of the Bible text data** is sequential and structured, organized into books, chapters, and verses. It contains a rich vocabulary and a distinct linguistic style. For a retrieval-based chatbot, relevant aspects of this data include:\n- **Text content:** The actual words and sentences that will be encoded into numerical representations.\n- **Structural information:** Book, chapter, and verse numbers, which can be used to identify and present the retrieved passages.\n- **Context:** The surrounding verses or chapters that provide context for a particular passage.\n\nAdapting this notebook will involve modifying the data loading to handle text, preprocessing the text into a format suitable for the neural network (e.g., using embeddings), and adjusting the network's input and output layers to handle these text representations for the retrieval task.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d290b52c"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the Bible text data and preprocess it for a deep learning model. This will involve tokenization, vocabulary creation, and converting text to numerical sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0487d847"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to load the Bible text data and then process it by tokenizing, creating a vocabulary, converting to sequences, and padding. This addresses all steps of the current subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "097191e1",
        "outputId": "b0762e88-8e89-4efe-9ae9-bf1417d922c9"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_bible_text(filepath):\n",
        "    \"\"\"Loads Bible text from a file.\"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Tokenizes, creates vocabulary, converts to sequences, and pads text.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize into words\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Create vocabulary\n",
        "    word_counts = Counter(tokens)\n",
        "    # Sort words by frequency\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    # Create word-to-index and index-to-word mappings\n",
        "    word_to_index = {word: index for index, word in enumerate(sorted_vocab, start=1)}\n",
        "    index_to_word = {index: word for word, index in word_to_index.items()}\n",
        "    # Add padding token\n",
        "    word_to_index['<pad>'] = 0\n",
        "    index_to_word[0] = '<pad>'\n",
        "\n",
        "    # Convert tokens to sequences of indices\n",
        "    sequences = [word_to_index.get(token, 0) for token in tokens] # Use 0 for unknown words\n",
        "\n",
        "    # Determine maximum sequence length (can be adjusted)\n",
        "    max_sequence_length = 100 # Example length\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences([sequences],\n",
        "                                                                      maxlen=max_sequence_length,\n",
        "                                                                      padding='post',\n",
        "                                                                      truncating='post')[0]\n",
        "\n",
        "    # Note: For a retrieval model, you would typically process text in chunks (e.g., verses)\n",
        "    # and store the sequences along with their original references (book, chapter, verse).\n",
        "    # For simplicity in this example, we process the entire text as one sequence and\n",
        "    # only return the vocabulary and one padded sequence. In a real application,\n",
        "    # you would create sequences for each verse/chunk.\n",
        "\n",
        "    return padded_sequences, word_to_index, index_to_word, max_sequence_length\n",
        "\n",
        "# Placeholder for loading actual Bible text. Replace with your file path.\n",
        "# Assuming 'bible.txt' is in the same directory or provide the full path.\n",
        "# In a real scenario, you would load and process the Bible verse by verse\n",
        "# and store mappings from verse references to their sequences.\n",
        "try:\n",
        "    bible_text = load_bible_text('bible.txt') # Replace with actual path\n",
        "    padded_sequence, word_to_index, index_to_word, max_len = preprocess_text(bible_text)\n",
        "\n",
        "    print(\"Vocabulary size:\", len(word_to_index))\n",
        "    print(\"First 10 words in vocabulary:\", list(word_to_index.keys())[:10])\n",
        "    print(\"Padded sequence shape:\", padded_sequence.shape)\n",
        "    # Display a small part of the padded sequence\n",
        "    print(\"First 20 elements of padded sequence:\", padded_sequence[:20])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Bible text file not found. Please provide the correct path.\")\n",
        "    # Create dummy data for subsequent steps if file not found\n",
        "    padded_sequence = np.zeros(100, dtype=int)\n",
        "    word_to_index = {'<pad>': 0, 'hello': 1, 'world': 2}\n",
        "    index_to_word = {0: '<pad>', 1: 'hello', 2: 'world'}\n",
        "    max_len = 100\n",
        "    print(\"Using dummy data for demonstration.\")\n",
        "\n",
        "# Store processed data and metadata for later use\n",
        "processed_bible_data = {\n",
        "    'padded_sequence': padded_sequence,\n",
        "    'word_to_index': word_to_index,\n",
        "    'index_to_word': index_to_word,\n",
        "    'max_sequence_length': max_len\n",
        "    # In a full retrieval model, you would also store\n",
        "    # 'verse_sequences': {verse_ref: sequence}\n",
        "    # 'verse_texts': {verse_ref: text}\n",
        "}"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bible text file not found. Please provide the correct path.\n",
            "Using dummy data for demonstration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f7cf32d"
      },
      "source": [
        "## Model architecture adaptation\n",
        "\n",
        "### Subtask:\n",
        "Modify the existing neural network architecture or introduce new layers (like RNNs, LSTMs, or Transformers) suitable for processing sequential text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4b91bd"
      },
      "source": [
        "## Training\n",
        "\n",
        "### Subtask:\n",
        "Adapt the training loop to handle sequential data and train the modified model on the preprocessed Bible text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd82e0ff"
      },
      "source": [
        "**Reasoning**:\n",
        "The next logical step is to modify the existing `model` function to train on sequential data. This involves adapting the function signature to accept the preprocessed data, and updating the training loop to handle the forward and backward propagation through the new, text-oriented architecture (embedding and potentially recurrent layers). The cost function and parameter updates will also need to be adjusted accordingly. Since a full RNN implementation from scratch is complex, I will focus on adapting the existing `model` function's structure to demonstrate the training loop's flow with sequential data, while keeping the core logic consistent with the notebook's from-scratch approach. This will provide a clear educational example of how the training loop would be modified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eccf6bc",
        "outputId": "645f29b3-2d44-41a5-a343-a514ffb4ef6a"
      },
      "source": [
        "def model_bible_chatbot(X_train_seq, Y_train_seq, word_to_index, embedding_dim, hidden_dim, output_dim, learning_rate=0.01, num_iterations=1000):\n",
        "    \"\"\"\n",
        "    Adapted model function for training a neural network on sequential Bible text data for a retrieval-based chatbot.\n",
        "\n",
        "    Args:\n",
        "        X_train_seq (np.array): Padded sequences of training text data.\n",
        "        Y_train_seq (np.array): Padded sequences of target text data (for retrieval, this could be the same as X_train_seq or related passages).\n",
        "        word_to_index (dict): Vocabulary mapping words to indices.\n",
        "        embedding_dim (int): The size of the word embeddings.\n",
        "        hidden_dim (int): The size of the hidden layer.\n",
        "        output_dim (int): The size of the output layer (should match embedding_dim for retrieval).\n",
        "        learning_rate (float): The learning rate for gradient descent.\n",
        "        num_iterations (int): The number of training iterations.\n",
        "\n",
        "    Returns:\n",
        "        dict: The trained parameters of the model.\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "    vocab_size = len(word_to_index)\n",
        "\n",
        "    # Initialize parameters for a simple sequential model (Embedding -> Hidden -> Output)\n",
        "    # This is a simplified representation of what would be needed for an RNN/LSTM from scratch.\n",
        "    parameters = {\n",
        "        \"W_embedding\": np.random.randn(embedding_dim, vocab_size) * 0.01,\n",
        "        \"W_hidden\": np.random.randn(hidden_dim, embedding_dim) * 0.01,\n",
        "        \"b_hidden\": np.zeros((hidden_dim, 1)),\n",
        "        \"W_output\": np.random.randn(output_dim, hidden_dim) * 0.01,\n",
        "        \"b_output\": np.zeros((output_dim, 1))\n",
        "    }\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Note: In a real scenario, you'd iterate through mini-batches of sequences.\n",
        "        # For simplicity, we process one sequence at a time in this example.\n",
        "\n",
        "        # --- Simplified Forward Propagation ---\n",
        "        # This is a conceptual representation. A true RNN would have a more complex forward pass.\n",
        "        # 1. Embedding lookup (conceptual)\n",
        "        # In a real implementation, you'd look up the embedding for each word in the sequence.\n",
        "        # Here, we'll average the embeddings of the input sequence for simplicity.\n",
        "        avg_embedding = np.mean(parameters[\"W_embedding\"][:, X_train_seq], axis=1, keepdims=True)\n",
        "\n",
        "        # 2. Hidden layer\n",
        "        Z_hidden = np.dot(parameters[\"W_hidden\"], avg_embedding) + parameters[\"b_hidden\"]\n",
        "        A_hidden = np.tanh(Z_hidden) # Using tanh as a common RNN activation\n",
        "\n",
        "        # 3. Output layer\n",
        "        Z_output = np.dot(parameters[\"W_output\"], A_hidden) + parameters[\"b_output\"]\n",
        "        # The output is the final embedding for the sequence\n",
        "        output_embedding = Z_output\n",
        "\n",
        "        # --- Simplified Cost Function (e.g., Mean Squared Error for retrieval) ---\n",
        "        # We want the output embedding to be close to a target embedding.\n",
        "        # For this example, we'll use the average embedding of the target sequence as the target.\n",
        "        target_embedding = np.mean(parameters[\"W_embedding\"][:, Y_train_seq], axis=1, keepdims=True)\n",
        "        cost = np.mean((output_embedding - target_embedding)**2)\n",
        "\n",
        "        # --- Simplified Backward Propagation ---\n",
        "        # This is a conceptual representation of backpropagation through time (BPTT).\n",
        "        # 1. Gradient of the cost with respect to the output\n",
        "        dZ_output = 2 * (output_embedding - target_embedding) / output_embedding.size\n",
        "\n",
        "        # 2. Gradients for the output layer\n",
        "        dW_output = np.dot(dZ_output, A_hidden.T)\n",
        "        db_output = dZ_output\n",
        "\n",
        "        # 3. Gradients for the hidden layer\n",
        "        dZ_hidden = np.dot(parameters[\"W_output\"].T, dZ_output) * (1 - np.power(A_hidden, 2))\n",
        "        dW_hidden = np.dot(dZ_hidden, avg_embedding.T)\n",
        "        db_hidden = dZ_hidden\n",
        "\n",
        "        # 4. Gradient for the embedding layer (highly simplified)\n",
        "        # In a real RNN, this gradient would be distributed back to the specific words' embeddings.\n",
        "        # Here, we'll apply the gradient to the average embedding's source.\n",
        "        # This is a major simplification for educational purposes.\n",
        "        dW_embedding = np.zeros_like(parameters[\"W_embedding\"])\n",
        "        grad_avg_embedding = np.dot(parameters[\"W_hidden\"].T, dZ_hidden)\n",
        "        for idx in X_train_seq:\n",
        "            dW_embedding[:, idx] += grad_avg_embedding.flatten() / len(X_train_seq)\n",
        "\n",
        "\n",
        "        # --- Update Parameters ---\n",
        "        parameters[\"W_output\"] -= learning_rate * dW_output\n",
        "        parameters[\"b_output\"] -= learning_rate * db_output\n",
        "        parameters[\"W_hidden\"] -= learning_rate * dW_hidden\n",
        "        parameters[\"b_hidden\"] -= learning_rate * db_hidden\n",
        "        parameters[\"W_embedding\"] -= learning_rate * dW_embedding\n",
        "\n",
        "        if i % (num_iterations / 10) == 0:\n",
        "            print(f\"Iteration {i}, Cost: {cost:.6f}\")\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Example usage with the dummy data\n",
        "embedding_dim = 50\n",
        "hidden_dim = 100\n",
        "# For retrieval, output_dim is typically the same as embedding_dim\n",
        "output_dim = embedding_dim\n",
        "\n",
        "# Using the dummy data from the preprocessing step\n",
        "X_train_seq = processed_bible_data['padded_sequence']\n",
        "# For a retrieval task, the target could be the same sequence (autoencoder-style)\n",
        "# or a related one. Here, we use the same sequence for simplicity.\n",
        "Y_train_seq = X_train_seq\n",
        "\n",
        "trained_parameters = model_bible_chatbot(X_train_seq, Y_train_seq,\n",
        "                                        processed_bible_data['word_to_index'],\n",
        "                                        embedding_dim, hidden_dim, output_dim,\n",
        "                                        learning_rate=0.1, num_iterations=2000)\n",
        "print(\"\\nTraining complete. Model parameters are ready for use in retrieval.\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Cost: 0.000081\n",
            "Iteration 200, Cost: 0.000016\n",
            "Iteration 400, Cost: 0.000003\n",
            "Iteration 600, Cost: 0.000001\n",
            "Iteration 800, Cost: 0.000000\n",
            "Iteration 1000, Cost: 0.000000\n",
            "Iteration 1200, Cost: 0.000000\n",
            "Iteration 1400, Cost: 0.000000\n",
            "Iteration 1600, Cost: 0.000000\n",
            "Iteration 1800, Cost: 0.000000\n",
            "\n",
            "Training complete. Model parameters are ready for use in retrieval.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7df6bf3b"
      },
      "source": [
        "## Text generation or retrieval\n",
        "\n",
        "### Subtask:\n",
        "Implement the logic for the chatbot to generate or retrieve relevant Bible text based on user input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7861e734"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic for generating text embeddings and retrieving relevant Bible verses using dummy data for demonstration purposes, since actual Bible data loading failed in a previous step. This involves defining `get_text_embedding` and `retrieve_bible_verse` functions as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa1683b4",
        "outputId": "20df093a-f0b9-498d-a4d7-7e0a6b1c4a7b"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Using the dummy data for word_to_index and max_len created in the preprocessing step\n",
        "word_to_index = processed_bible_data['word_to_index']\n",
        "max_len = processed_bible_data['max_sequence_length']\n",
        "trained_parameters = trained_parameters # Use the dummy trained parameters\n",
        "\n",
        "def get_text_embedding(text, word_to_index, parameters, max_len):\n",
        "    \"\"\"\n",
        "    Generates an embedding for a given text string using the trained model parameters.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string (user query or Bible verse).\n",
        "        word_to_index (dict): Vocabulary mapping words to indices.\n",
        "        parameters (dict): Trained model parameters (including W_embedding, W_hidden, b_hidden, W_output, b_output).\n",
        "        max_len (int): Maximum sequence length for padding.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The generated embedding vector for the text.\n",
        "    \"\"\"\n",
        "    # Preprocess the input text\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Convert tokens to sequences of indices\n",
        "    sequences = [word_to_index.get(token, 0) for token in tokens]\n",
        "\n",
        "    # Pad the sequence\n",
        "    # Use np.pad for padding to match the from-scratch implementation context\n",
        "    if len(sequences) < max_len:\n",
        "        padded_sequence = np.pad(sequences, (0, max_len - len(sequences)), 'constant', constant_values=0)\n",
        "    else:\n",
        "        padded_sequence = np.array(sequences[:max_len]) # Truncate if longer\n",
        "\n",
        "    # Ensure padded_sequence is a 1D array of integers\n",
        "    padded_sequence = padded_sequence.astype(int)\n",
        "\n",
        "    # --- Simplified Forward Pass to get embedding ---\n",
        "    # This mirrors the conceptual forward pass in model_bible_chatbot\n",
        "\n",
        "    # 1. Conceptual Embedding lookup and averaging\n",
        "    # Select the embeddings for the indices in the padded sequence\n",
        "    selected_embeddings = parameters[\"W_embedding\"][:, padded_sequence]\n",
        "    # Calculate the average embedding\n",
        "    avg_embedding = np.mean(selected_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    # 2. Hidden layer\n",
        "    Z_hidden = np.dot(parameters[\"W_hidden\"], avg_embedding) + parameters[\"b_hidden\"]\n",
        "    A_hidden = np.tanh(Z_hidden)\n",
        "\n",
        "    # 3. Output layer\n",
        "    Z_output = np.dot(parameters[\"W_output\"], A_hidden) + parameters[\"b_output\"]\n",
        "    # The output is the final embedding for the sequence\n",
        "    text_embedding = Z_output\n",
        "\n",
        "    return text_embedding\n",
        "\n",
        "def retrieve_bible_verse(query, processed_bible_data, parameters, similarity_metric='cosine'):\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant Bible verse(s) based on a user query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's input query.\n",
        "        processed_bible_data (dict): Dictionary containing processed Bible data.\n",
        "                                     For this demo, it will contain dummy verse texts and embeddings.\n",
        "        parameters (dict): Trained model parameters.\n",
        "        similarity_metric (str): The metric to use for calculating similarity (e.g., 'cosine').\n",
        "\n",
        "    Returns:\n",
        "        str: The text of the most relevant Bible verse(s).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the user query\n",
        "    query_embedding = get_text_embedding(query, processed_bible_data['word_to_index'], parameters, processed_bible_data['max_sequence_length'])\n",
        "\n",
        "    # --- Use Dummy Bible Verse Embeddings ---\n",
        "    # Since actual processing failed, use dummy data to demonstrate retrieval logic.\n",
        "    dummy_verse_texts = {\n",
        "        \"Genesis 1:1\": \"In the beginning God created the heaven and the earth.\",\n",
        "        \"John 3:16\": \"For God so loved the world, that he gave his only begotten Son, that whosoever believeth in him should not perish, but have everlasting life.\",\n",
        "        \"Psalm 23:1\": \"The Lord is my shepherd; I shall not want.\"\n",
        "    }\n",
        "    # Create dummy embeddings for these verses.\n",
        "    # In a real application, these would be pre-computed using get_text_embedding\n",
        "    # for each actual Bible verse during the preprocessing/indexing phase.\n",
        "    embedding_dim = parameters['W_output'].shape[0] # Get embedding dim from trained parameters\n",
        "    dummy_verse_embeddings = {\n",
        "        verse_ref: np.random.randn(embedding_dim, 1) for verse_ref in dummy_verse_texts.keys()\n",
        "    }\n",
        "\n",
        "    # Calculate similarity between the query embedding and each dummy verse embedding\n",
        "    similarities = {}\n",
        "    for verse_ref, verse_embedding in dummy_verse_embeddings.items():\n",
        "        if similarity_metric == 'cosine':\n",
        "            # Cosine similarity: (A . B) / (||A|| ||B||)\n",
        "            # Ensure embeddings are treated as vectors (flatten if necessary)\n",
        "            query_vec = query_embedding.flatten()\n",
        "            verse_vec = verse_embedding.flatten()\n",
        "            similarity = np.dot(query_vec, verse_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(verse_vec))\n",
        "        # Add other similarity metrics here if needed\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported similarity metric.\")\n",
        "\n",
        "        similarities[verse_ref] = similarity\n",
        "\n",
        "    # Find the verse(s) with the highest similarity score\n",
        "    # Sort verses by similarity in descending order\n",
        "    sorted_verses = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Return the text of the most relevant verse\n",
        "    most_relevant_verse_ref = sorted_verses[0][0]\n",
        "    return dummy_verse_texts[most_relevant_verse_ref]\n",
        "\n",
        "# --- Demonstrate Retrieval with Dummy Data ---\n",
        "user_query = \"Tell me about creation\"\n",
        "retrieved_verse = retrieve_bible_verse(user_query, processed_bible_data, trained_parameters, similarity_metric='cosine')\n",
        "\n",
        "print(\"\\nUser Query:\", user_query)\n",
        "print(\"Retrieved Verse:\", retrieved_verse)\n",
        "\n",
        "user_query_2 = \"What does the Bible say about love?\"\n",
        "retrieved_verse_2 = retrieve_bible_verse(user_query_2, processed_bible_data, trained_parameters, similarity_metric='cosine')\n",
        "\n",
        "print(\"\\nUser Query:\", user_query_2)\n",
        "print(\"Retrieved Verse:\", retrieved_verse_2)\n",
        "\n",
        "user_query_3 = \"About a shepherd\"\n",
        "retrieved_verse_3 = retrieve_bible_verse(user_query_3, processed_bible_data, trained_parameters, similarity_metric='cosine')\n",
        "\n",
        "print(\"\\nUser Query:\", user_query_3)\n",
        "print(\"Retrieved Verse:\", retrieved_verse_3)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: Tell me about creation\n",
            "Retrieved Verse: The Lord is my shepherd; I shall not want.\n",
            "\n",
            "User Query: What does the Bible say about love?\n",
            "Retrieved Verse: For God so loved the world, that he gave his only begotten Son, that whosoever believeth in him should not perish, but have everlasting life.\n",
            "\n",
            "User Query: About a shepherd\n",
            "Retrieved Verse: For God so loved the world, that he gave his only begotten Son, that whosoever believeth in him should not perish, but have everlasting life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852c605d"
      },
      "source": [
        "**Reasoning**:\n",
        "Add educational comments to the code cell that implements the logic for generating text embeddings and retrieving relevant Bible verses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0a7edc3",
        "outputId": "00fe760a-33ad-4550-b1d4-e042def53135"
      },
      "source": [
        "# Import necessary libraries (numpy, re, string, Counter already imported)\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Using the dummy data for word_to_index and max_len created in the preprocessing step\n",
        "# These variables are assumed to be available from previous cells where preprocessing was attempted.\n",
        "word_to_index = processed_bible_data['word_to_index']\n",
        "max_len = processed_bible_data['max_sequence_length']\n",
        "# Use the dummy trained parameters obtained from the model_bible_chatbot function.\n",
        "trained_parameters = trained_parameters # Use the dummy trained parameters\n",
        "\n",
        "def get_text_embedding(text, word_to_index, parameters, max_len):\n",
        "    \"\"\"\n",
        "    Generates an embedding for a given text string using the trained model parameters.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string (user query or Bible verse).\n",
        "        word_to_index (dict): Vocabulary mapping words to indices.\n",
        "        parameters (dict): Trained model parameters (including W_embedding, W_hidden, b_hidden, W_output, b_output).\n",
        "        max_len (int): Maximum sequence length for padding.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The generated embedding vector for the text.\n",
        "    \"\"\"\n",
        "    # Preprocess the input text: convert to lowercase and remove punctuation.\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize the text into individual words.\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Convert tokens to sequences of indices using the word_to_index vocabulary.\n",
        "    # Words not found in the vocabulary are mapped to index 0 ('<pad>').\n",
        "    sequences = [word_to_index.get(token, 0) for token in tokens]\n",
        "\n",
        "    # Pad the sequence to a fixed maximum length (max_len).\n",
        "    # This ensures all input sequences have the same dimension.\n",
        "    # Use np.pad for padding to match the from-scratch implementation context\n",
        "    if len(sequences) < max_len:\n",
        "        # Pad with zeros at the end if the sequence is shorter than max_len.\n",
        "        padded_sequence = np.pad(sequences, (0, max_len - len(sequences)), 'constant', constant_values=0)\n",
        "    else:\n",
        "        # Truncate the sequence if it is longer than max_len.\n",
        "        padded_sequence = np.array(sequences[:max_len]) # Truncate if longer\n",
        "\n",
        "    # Ensure padded_sequence is a 1D array of integers, as expected by subsequent steps.\n",
        "    padded_sequence = padded_sequence.astype(int)\n",
        "\n",
        "    # --- Simplified Forward Pass to get embedding ---\n",
        "    # This section performs a forward pass through the relevant parts of the model\n",
        "    # to generate a numerical representation (embedding) for the input text sequence.\n",
        "    # This mirrors the conceptual forward pass in model_bible_chatbot.\n",
        "\n",
        "    # 1. Conceptual Embedding lookup and averaging\n",
        "    # Select the embedding vectors for each word index in the padded sequence from the embedding matrix.\n",
        "    selected_embeddings = parameters[\"W_embedding\"][:, padded_sequence]\n",
        "    # Calculate the average of these embedding vectors across all words in the sequence.\n",
        "    # This simple averaging is a conceptual step; real RNNs/LSTMs process sequences iteratively.\n",
        "    avg_embedding = np.mean(selected_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    # 2. Hidden layer computation\n",
        "    # Calculate the pre-activation value for the hidden layer.\n",
        "    Z_hidden = np.dot(parameters[\"W_hidden\"], avg_embedding) + parameters[\"b_hidden\"]\n",
        "    # Apply the tanh activation function to get the hidden layer's activation.\n",
        "    A_hidden = np.tanh(Z_hidden) # Using tanh as a common RNN activation\n",
        "\n",
        "    # 3. Output layer computation\n",
        "    # Calculate the pre-activation value for the output layer.\n",
        "    Z_output = np.dot(parameters[\"W_output\"], A_hidden) + parameters[\"b_output\"]\n",
        "    # The output of this layer is considered the final embedding for the input sequence.\n",
        "    text_embedding = Z_output\n",
        "\n",
        "    # Return the computed embedding vector.\n",
        "    return text_embedding\n",
        "\n",
        "def retrieve_bible_verse(query, processed_bible_data, parameters, similarity_metric='cosine'):\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant Bible verse(s) based on a user query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's input query.\n",
        "        processed_bible_data (dict): Dictionary containing processed Bible data.\n",
        "                                     For this demo, it will contain dummy verse texts and embeddings.\n",
        "        parameters (dict): Trained model parameters.\n",
        "        similarity_metric (str): The metric to use for calculating similarity (e.g., 'cosine').\n",
        "\n",
        "    Returns:\n",
        "        str: The text of the most relevant Bible verse(s).\n",
        "    \"\"\"\n",
        "    # Generate an embedding for the user query using the get_text_embedding function.\n",
        "    query_embedding = get_text_embedding(query, processed_bible_data['word_to_index'], parameters, processed_bible_data['max_sequence_length'])\n",
        "\n",
        "    # --- Use Dummy Bible Verse Embeddings ---\n",
        "    # Since actual Bible text loading and processing failed in a previous step,\n",
        "    # we are using dummy data to demonstrate the retrieval logic.\n",
        "    dummy_verse_texts = {\n",
        "        \"Genesis 1:1\": \"In the beginning God created the heaven and the earth.\",\n",
        "        \"John 3:16\": \"For God so loved the world, that he gave his only begotten Son, that whosoever believeth in him should not perish, but have everlasting life.\",\n",
        "        \"Psalm 23:1\": \"The Lord is my shepherd; I shall not want.\"\n",
        "    }\n",
        "    # Create dummy embeddings for these verses.\n",
        "    # In a real application, these would be pre-computed and stored during\n",
        "    # the data preprocessing/indexing phase using the get_text_embedding function\n",
        "    # for each actual Bible verse.\n",
        "    embedding_dim = parameters['W_output'].shape[0] # Get embedding dim from trained parameters\n",
        "    dummy_verse_embeddings = {\n",
        "        verse_ref: np.random.randn(embedding_dim, 1) for verse_ref in dummy_verse_texts.keys()\n",
        "    }\n",
        "\n",
        "    # Calculate the similarity between the query embedding and each dummy verse embedding.\n",
        "    similarities = {}\n",
        "    for verse_ref, verse_embedding in dummy_verse_embeddings.items():\n",
        "        # Check the specified similarity metric.\n",
        "        if similarity_metric == 'cosine':\n",
        "            # Compute cosine similarity. Cosine similarity measures the cosine of the angle\n",
        "            # between two vectors, indicating how similar their direction is.\n",
        "            # Formula: (A . B) / (||A|| ||B||)\n",
        "            # Flatten the embeddings to 1D vectors for dot product and norm calculations.\n",
        "            query_vec = query_embedding.flatten()\n",
        "            verse_vec = verse_embedding.flatten()\n",
        "            # Handle potential division by zero if embeddings are zero vectors\n",
        "            norm_query = np.linalg.norm(query_vec)\n",
        "            norm_verse = np.linalg.norm(verse_vec)\n",
        "            if norm_query == 0 or norm_verse == 0:\n",
        "                similarity = 0 # Or handle as appropriate for your application\n",
        "            else:\n",
        "                similarity = np.dot(query_vec, verse_vec) / (norm_query * norm_verse)\n",
        "        # Add logic for other similarity metrics (e.g., Euclidean distance) here if needed.\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported similarity metric. Only 'cosine' is supported in this example.\")\n",
        "\n",
        "        # Store the computed similarity score for the current verse.\n",
        "        similarities[verse_ref] = similarity\n",
        "\n",
        "    # Find the verse(s) with the highest similarity score(s).\n",
        "    # Sort the verses based on their similarity scores in descending order.\n",
        "    sorted_verses = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Return the text of the most relevant verse (the one with the highest similarity).\n",
        "    most_relevant_verse_ref = sorted_verses[0][0]\n",
        "    return dummy_verse_texts[most_relevant_verse_ref]\n",
        "\n",
        "# --- Demonstrate Retrieval with Dummy Data ---\n",
        "# This section shows how the retrieve_bible_verse function would be used with example queries.\n",
        "# Note: Since dummy embeddings are random, the retrieved verses will not be meaningful\n",
        "# in relation to the query, but this demonstrates the retrieval mechanism.\n",
        "\n",
        "user_query = \"Tell me about creation\"\n",
        "retrieved_verse = retrieve_bible_verse(user_query, processed_bible_data, trained_parameters, similarity_metric='cosine')\n",
        "\n",
        "print(\"\\nUser Query:\", user_query)\n",
        "print(\"Retrieved Verse:\", retrieved_verse)\n",
        "\n",
        "user_query_2 = \"What does the Bible say about love?\"\n",
        "retrieved_verse_2 = retrieve_bible_verse(user_query_2, processed_bible_data, trained_parameters, similarity_metric='cosine')\n",
        "\n",
        "print(\"\\nUser Query:\", user_query_2)\n",
        "print(\"Retrieved Verse:\", retrieved_verse_2)\n",
        "\n",
        "user_query_3 = \"About a shepherd\"\n",
        "retrieved_verse_3 = retrieve_bible_verse(user_query_3, processed_bible_data, trained_parameters, similarity_metric='cosine')\n",
        "\n",
        "print(\"\\nUser Query:\", user_query_3)\n",
        "retrieved_verse_3 = retrieve_bible_verse(user_query_3, processed_bible_data, trained_parameters, similarity_metric='cosine')\n",
        "\n",
        "print(\"\\nUser Query:\", user_query_3)\n",
        "print(\"Retrieved Verse:\", retrieved_verse_3)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: Tell me about creation\n",
            "Retrieved Verse: For God so loved the world, that he gave his only begotten Son, that whosoever believeth in him should not perish, but have everlasting life.\n",
            "\n",
            "User Query: What does the Bible say about love?\n",
            "Retrieved Verse: For God so loved the world, that he gave his only begotten Son, that whosoever believeth in him should not perish, but have everlasting life.\n",
            "\n",
            "User Query: About a shepherd\n",
            "\n",
            "User Query: About a shepherd\n",
            "Retrieved Verse: The Lord is my shepherd; I shall not want.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de2bc7be"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "### Subtask:\n",
        "Define metrics and evaluate the performance of the chatbot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0a8fc4a"
      },
      "source": [
        "**Reasoning**:\n",
        "Add a markdown cell to explain the evaluation metrics for the chatbot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "9b9edd4a",
        "outputId": "3f133b4a-25e1-4080-f6fb-6d8ada6a9d6e"
      },
      "source": [
        "%%markdown\n",
        "# Chatbot Evaluation\n",
        "\n",
        "For a retrieval-based chatbot like the one outlined, evaluating its performance primarily involves assessing how relevant the retrieved Bible passages are to a user's query. Since the chatbot's goal is to find the most appropriate existing text, we focus on metrics from information retrieval.\n",
        "\n",
        "Relevant evaluation metrics include:\n",
        "\n",
        "-   **Precision@K**: Measures the proportion of the top K retrieved passages that are actually relevant to the query. A high Precision@K indicates that when the chatbot returns a small number of results, a large fraction of them are useful.\n",
        "-   **Recall@K**: Measures the proportion of all known relevant passages for a query that are included within the top K retrieved passages. High Recall@K means the chatbot finds a large percentage of all possible relevant answers within its top results.\n",
        "-   **Mean Average Precision (MAP)**: This metric considers the order of the retrieved passages. For each query, it calculates the Average Precision (AP), which is the average of the precision values calculated at each point where a relevant document is retrieved. MAP is then the average of the AP over all queries. It gives a single-number measure that considers both precision and recall, and the ranking of relevant documents.\n",
        "-   **Mean Reciprocal Rank (MRR)**: Typically used when there is only one correct or relevant answer for each query. It measures the reciprocal of the rank of the first relevant passage retrieved. If the first retrieved passage is relevant, MRR is 1. If the second is the first relevant one, MRR is 1/2, and so on. Higher MRR indicates that the first relevant result appears earlier in the ranked list.\n",
        "-   **Human Evaluation**: While quantitative metrics are useful, the most reliable way to evaluate a chatbot's helpfulness and the relevance of its responses is through human judgment. Human evaluators can assess the quality, context, and understandability of the retrieved passages in response to diverse queries.\n",
        "\n",
        "Implementing these metrics requires a dedicated **test dataset**. This dataset should consist of a collection of user queries, each paired with a list of known relevant Bible passages. The current notebook does not include such a dataset.\n",
        "\n",
        "Due to the lack of a suitable test dataset and the use of dummy data in the preceding steps, a practical, quantitative evaluation of the chatbot's retrieval performance cannot be performed within this notebook.\n",
        "\n",
        "In a real-world scenario, a comprehensive evaluation phase using these metrics and a properly curated test dataset is essential to understand the chatbot's effectiveness, identify areas for improvement, and compare different model variations or retrieval strategies."
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Chatbot Evaluation\n\nFor a retrieval-based chatbot like the one outlined, evaluating its performance primarily involves assessing how relevant the retrieved Bible passages are to a user's query. Since the chatbot's goal is to find the most appropriate existing text, we focus on metrics from information retrieval.\n\nRelevant evaluation metrics include:\n\n-   **Precision@K**: Measures the proportion of the top K retrieved passages that are actually relevant to the query. A high Precision@K indicates that when the chatbot returns a small number of results, a large fraction of them are useful.\n-   **Recall@K**: Measures the proportion of all known relevant passages for a query that are included within the top K retrieved passages. High Recall@K means the chatbot finds a large percentage of all possible relevant answers within its top results.\n-   **Mean Average Precision (MAP)**: This metric considers the order of the retrieved passages. For each query, it calculates the Average Precision (AP), which is the average of the precision values calculated at each point where a relevant document is retrieved. MAP is then the average of the AP over all queries. It gives a single-number measure that considers both precision and recall, and the ranking of relevant documents.\n-   **Mean Reciprocal Rank (MRR)**: Typically used when there is only one correct or relevant answer for each query. It measures the reciprocal of the rank of the first relevant passage retrieved. If the first retrieved passage is relevant, MRR is 1. If the second is the first relevant one, MRR is 1/2, and so on. Higher MRR indicates that the first relevant result appears earlier in the ranked list.\n-   **Human Evaluation**: While quantitative metrics are useful, the most reliable way to evaluate a chatbot's helpfulness and the relevance of its responses is through human judgment. Human evaluators can assess the quality, context, and understandability of the retrieved passages in response to diverse queries.\n\nImplementing these metrics requires a dedicated **test dataset**. This dataset should consist of a collection of user queries, each paired with a list of known relevant Bible passages. The current notebook does not include such a dataset.\n\nDue to the lack of a suitable test dataset and the use of dummy data in the preceding steps, a practical, quantitative evaluation of the chatbot's retrieval performance cannot be performed within this notebook.\n\nIn a real-world scenario, a comprehensive evaluation phase using these metrics and a properly curated test dataset is essential to understand the chatbot's effectiveness, identify areas for improvement, and compare different model variations or retrieval strategies.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b7997ce"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The project aims to build a retrieval-based Bible chatbot, which involves indexing Bible text, converting queries and passages into numerical representations (embeddings), and retrieving the most relevant passages based on similarity.\n",
        "*   The Bible text data is sequential and structured, requiring preprocessing steps like tokenization, vocabulary creation, and converting text to numerical sequences with padding.\n",
        "*   The neural network architecture needs to be adapted to handle sequential text, ideally incorporating an embedding layer and recurrent layers (like LSTMs or GRUs) to generate sequence embeddings for retrieval.\n",
        "*   Training involves adapting a loop to process sequential data, aiming to learn embeddings that facilitate finding relevant passages based on query similarity.\n",
        "*   Chatbot retrieval logic involves generating an embedding for a user query and comparing it using similarity metrics (e.g., cosine similarity) against pre-computed embeddings of Bible verses to find the most relevant match.\n",
        "*   Evaluation of a retrieval-based chatbot relies on information retrieval metrics such as Precision@K, Recall@K, Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR, if applicable), ideally supplemented by human evaluation, all of which require a dedicated test dataset.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current notebook provides a conceptual framework and simplified implementation. The next crucial step is to obtain and process the actual Bible text data verse-by-verse to build a proper index of verse embeddings.\n",
        "*   For a more robust model, replace the simplified from-scratch network with a standard deep learning library (like TensorFlow or PyTorch) and implement a more sophisticated sequential model architecture (e.g., an LSTM or Transformer encoder) for generating text embeddings.\n"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "TSPse",
      "launcher_item_id": "24mxX"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}